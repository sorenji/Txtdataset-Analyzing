{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件读取结束\n",
      "文件预处理结束\n",
      "文件特征选择结束\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件训练结束\n",
      "在测试集中准确率为：0.6788888888888889\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python  \n",
    "# -*- coding: UTF-8 -*-  \n",
    "#pip install jieba\n",
    "\n",
    "import os\n",
    "import importlib,sys\n",
    "importlib.reload(sys)  \n",
    "#sys.setdefaultencoding('utf-8')\n",
    "import jieba #结巴分词 这是一个包，在之前需要安装 此代码中式精准模式\n",
    "#sklearn库对数据分析做了功能的集成 安装https://scikit-learn.org/stable/\n",
    "from sklearn import datasets#datasets读取要分析的数据\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #CountVectorizer中文文档分词后笔算每个词模型成文本向量模型 TfidfTransformer每个词权重计算\n",
    "from sklearn.svm import SVC #SVM分类算法 \n",
    "from sklearn.ensemble import RandomForestClassifier#随机森林分类算法\n",
    "from sklearn.feature_selection import chi2,SelectKBest #特则选择——卡方chi2\n",
    "import pickle\n",
    "\n",
    "stop_words_path = \"E:/Subject/Txtdatasets Analzing/ExampleCode/stopwords-zh.txt\"#stopwords的内容，即后面使用的删减stopwords，此处是相对路径，可用绝对路径\n",
    "\n",
    "#获取停用词数据集\n",
    "#可手动添加一些停用词\n",
    "def read_stopwordset():#读取所有行 去掉换行符\n",
    "    with open(stop_words_path, \"r\",encoding='UTF-8') as fp: \n",
    "        lines = fp.readlines()\n",
    "        words = [' ','\\r\\n']#words先添加了一些停用词内容\n",
    "        for line in lines:\n",
    "            words.append(line.strip())#将读取的内容加在Words 上\n",
    "    return words\n",
    "\n",
    "#利用结巴分词器对文本内容进行分词\n",
    "#输入要分词的句子\n",
    "#获得分词结果，结果以列表形式返回\n",
    "def fenci(sentence): \n",
    "    wordlist = jieba.lcut(sentence,cut_all=False,HMM=True) # jieba分词的lcut精确模式\n",
    "    return wordlist#返回lcut中Wordlist列表\n",
    "\n",
    "#将模型按照model_name进行保存\n",
    "def save_model(model_name,model):\n",
    "    feature_path = 'E:/Subject/Txtdatasets Analzing/models/'+model_name+'.pkl'#这是一个相对路径\n",
    "    with open(feature_path, 'wb') as fw:\n",
    "        pickle.dump(model, fw)\n",
    "\n",
    "#将模型按照model_name进行读取\n",
    "def load_model(model_name):\n",
    "    tfidftransformer_path = 'E:/Subject/Txtdatasets Analzing/models/'+model_name+'.pkl'\n",
    "    return pickle.load(open(tfidftransformer_path, \"rb\"))\n",
    "    \n",
    "#获取停用词\n",
    "stopwords = read_stopwordset()\n",
    "#按类别获取文档内容\n",
    "rawData = datasets.load_files(\"E:/Subject/Txtdatasets Analzing/Datacluster/train\",encoding = 'gbk',decode_error='ignore')#读取文件—名称 编码 错误解决方案  按类别获取文档\n",
    "print(\"文件读取结束\")\n",
    "X_train = [] #文本内容 定义\n",
    "Y_train = [] #文本标签\n",
    "count = [0,0,0,0,0,0,0,0,0] #控制训练集 样例处理的时候只读取十个文件即可 \n",
    "#过滤文件和分词\n",
    "for i in range(len(rawData.data)):\n",
    "    if rawData.filenames[i][-3:]=='txt':#判断文件名后缀，是则操作\n",
    "        if count[rawData.target[i]]<500:#每个类别采样量（调试可以为10\n",
    "            content = rawData.data[i]#第i篇文档的内容\n",
    "            result = fenci(content)#内容分词 结果是词的list\n",
    "            X_train.append(' '.join(result))#变成字符串 每个词以空格形式分开——得到训练集内容\n",
    "            Y_train.append(rawData.target[i])#第i篇文档类别的标签传给Y\n",
    "            count[rawData.target[i]]+=1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "print(\"文件预处理结束\")\n",
    "\n",
    "#将文档内容转换为词-词频的模式（文档向量模型\n",
    "count_vec = CountVectorizer(min_df=0.1,stop_words=stopwords) #可以利用最大最小文档频率对词进行筛选max_df=0.6,min_df=0.2\n",
    "#去除stopwords\n",
    "X_train = count_vec.fit_transform(X_train)#fit_transform是CountVectorizer的方法 \n",
    "#输出为文档向量模型\n",
    "\n",
    "save_model('count_vec',count_vec.vocabulary_)#保存模型count_vec\n",
    "\n",
    "#基于卡方的特征选择\n",
    "select_chi2 = SelectKBest(chi2,k=100)#卡方只是为特征评分 SelectKBest选K个最好的特征\n",
    "X_train = select_chi2.fit_transform(X_train,Y_train)\n",
    "save_model('select_chi2',select_chi2)\n",
    "\n",
    "#tfidf对词赋予权重\n",
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "save_model('tfidf',tfidf)\n",
    "#可以由LSA处理，sklean中也有 对特征的优化处理；SVD可以矩阵分解；可以想哪一步哪个时候执行更好\n",
    "print(\"文件特征选择结束\")\n",
    "\n",
    "#利用SVM算法对文档内容进行分类学习\n",
    "clf = SVC()#SVC+SVR=SVM  分类算法训练\n",
    "clf.fit(X_train, Y_train)#传入参数\n",
    "save_model('svm',clf)\n",
    "\n",
    "print(\"文件训练结束\")\n",
    "\n",
    "#在测试集中进行测试\n",
    "\n",
    "rawData = datasets.load_files(\"E:/Subject/Txtdatasets Analzing/Datacluster/test\",encoding = 'gbk',decode_error='ignore')#读取数据集\n",
    "X_test = []#定义两个数列\n",
    "Y_test = []\n",
    "#分词\n",
    "count = [0,0,0,0,0,0,0,0,0]\n",
    "for i in range(len(rawData.data)):\n",
    "    if rawData.filenames[i][-3:]=='txt':\n",
    "        if count[rawData.target[i]]<100:\n",
    "            content = rawData.data[i]\n",
    "            result = fenci(content)\n",
    "            X_test.append(' '.join(result))\n",
    "            Y_test.append(rawData.target[i])\n",
    "            count[rawData.target[i]]+=1\n",
    "        else:\n",
    "            continue\n",
    "#确保若训练用TFIDF，则测试也要，比如特征选择、转换等\n",
    "X_test = count_vec.transform(X_test)#执行\n",
    "X_test = select_chi2.transform(X_test)#卡方\n",
    "test_tfidf = tfidf.transform(X_test)\n",
    "Y_result = clf.predict(test_tfidf)#predict SVM\n",
    "\n",
    "#计算准确率\n",
    "correct = 0.0\n",
    "for i in range(len(Y_result)):\n",
    "    if Y_test[i] == Y_result[i]:#遍历Y的值确定准确率\n",
    "        correct+=1.0\n",
    "print(\"在测试集中准确率为：\"+str(correct/len(Y_result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
